{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from os import listdir\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePreprocessor:\n",
    "    \"\"\"The abstract class for a preprocessor. You should subclass\n",
    "    this and implement the methods actions and result, and possibly\n",
    "    __init__, goal_test, and path_cost. Then you will create instances\n",
    "    of your subclass and solve them with the various search functions.\"\"\"\n",
    "    \n",
    "    # List of contractions.\n",
    "    CONTRACTION_LIST = {\n",
    "            \"ain't\": \"is not\",\n",
    "            \"aren't\": \"are not\",\n",
    "            \"can't\": \"cannot\",\n",
    "            \"can't've\": \"cannot have\",\n",
    "            \"'cause\": \"because\",\n",
    "            \"could've\": \"could have\",\n",
    "            \"couldn't\": \"could not\",\n",
    "            \"couldn't've\": \"could not have\",\n",
    "            \"didn't\": \"did not\",\n",
    "            \"doesn't\": \"does not\",\n",
    "            \"don't\": \"do not\",\n",
    "            \"hadn't\": \"had not\",\n",
    "            \"hadn't've\": \"had not have\",\n",
    "            \"hasn't\": \"has not\",\n",
    "            \"haven't\": \"have not\",\n",
    "            \"he'd\": \"he would\",\n",
    "            \"he'd've\": \"he would have\",\n",
    "            \"he'll\": \"he will\",\n",
    "            \"he'll've\": \"he he will have\",\n",
    "            \"he's\": \"he is\",\n",
    "            \"how'd\": \"how did\",\n",
    "            \"how'd'y\": \"how do you\",\n",
    "            \"how'll\": \"how will\",\n",
    "            \"how's\": \"how is\",\n",
    "            \"I'd\": \"I would\",\n",
    "            \"I'd've\": \"I would have\",\n",
    "            \"I'll\": \"I will\",\n",
    "            \"I'll've\": \"I will have\",\n",
    "            \"I'm\": \"I am\",\n",
    "            \"I've\": \"I have\",\n",
    "            \"i'd\": \"i would\",\n",
    "            \"i'd've\": \"i would have\",\n",
    "            \"i'll\": \"i will\",\n",
    "            \"i'll've\": \"i will have\",\n",
    "            \"i'm\": \"i am\",\n",
    "            \"i've\": \"i have\",\n",
    "            \"isn't\": \"is not\",\n",
    "            \"it'd\": \"it would\",\n",
    "            \"it'd've\": \"it would have\",\n",
    "            \"it'll\": \"it will\",\n",
    "            \"it'll've\": \"it will have\",\n",
    "            \"it's\": \"it is\",\n",
    "            \"let's\": \"let us\",\n",
    "            \"ma'am\": \"madam\",\n",
    "            \"mayn't\": \"may not\",\n",
    "            \"might've\": \"might have\",\n",
    "            \"mightn't\": \"might not\",\n",
    "            \"mightn't've\": \"might not have\",\n",
    "            \"must've\": \"must have\",\n",
    "            \"mustn't\": \"must not\",\n",
    "            \"mustn't've\": \"must not have\",\n",
    "            \"needn't\": \"need not\",\n",
    "            \"needn't've\": \"need not have\",\n",
    "            \"o'clock\": \"of the clock\",\n",
    "            \"oughtn't\": \"ought not\",\n",
    "            \"oughtn't've\": \"ought not have\",\n",
    "            \"shan't\": \"shall not\",\n",
    "            \"sha'n't\": \"shall not\",\n",
    "            \"shan't've\": \"shall not have\",\n",
    "            \"she'd\": \"she would\",\n",
    "            \"she'd've\": \"she would have\",\n",
    "            \"she'll\": \"she will\",\n",
    "            \"she'll've\": \"she will have\",\n",
    "            \"she's\": \"she is\",\n",
    "            \"should've\": \"should have\",\n",
    "            \"shouldn't\": \"should not\",\n",
    "            \"shouldn't've\": \"should not have\",\n",
    "            \"so've\": \"so have\",\n",
    "            \"so's\": \"so as\",\n",
    "            \"that'd\": \"that would\",\n",
    "            \"that'd've\": \"that would have\",\n",
    "            \"that's\": \"that is\",\n",
    "            \"there'd\": \"there would\",\n",
    "            \"there'd've\": \"there would have\",\n",
    "            \"there's\": \"there is\",\n",
    "            \"they'd\": \"they would\",\n",
    "            \"they'd've\": \"they would have\",\n",
    "            \"they'll\": \"they will\",\n",
    "            \"they'll've\": \"they will have\",\n",
    "            \"they're\": \"they are\",\n",
    "            \"they've\": \"they have\",\n",
    "            \"to've\": \"to have\",\n",
    "            \"wasn't\": \"was not\",\n",
    "            \"we'd\": \"we would\",\n",
    "            \"we'd've\": \"we would have\",\n",
    "            \"we'll\": \"we will\",\n",
    "            \"we'll've\": \"we will have\",\n",
    "            \"we're\": \"we are\",\n",
    "            \"we've\": \"we have\",\n",
    "            \"weren't\": \"were not\",\n",
    "            \"what'll\": \"what will\",\n",
    "            \"what'll've\": \"what will have\",\n",
    "            \"what're\": \"what are\",\n",
    "            \"what's\": \"what is\",\n",
    "            \"what've\": \"what have\",\n",
    "            \"when's\": \"when is\",\n",
    "            \"when've\": \"when have\",\n",
    "            \"where'd\": \"where did\",\n",
    "            \"where's\": \"where is\",\n",
    "            \"where've\": \"where have\",\n",
    "            \"who'll\": \"who will\",\n",
    "            \"who'll've\": \"who will have\",\n",
    "            \"who's\": \"who is\",\n",
    "            \"who've\": \"who have\",\n",
    "            \"why's\": \"why is\",\n",
    "            \"why've\": \"why have\",\n",
    "            \"will've\": \"will have\",\n",
    "            \"won't\": \"will not\",\n",
    "            \"won't've\": \"will not have\",\n",
    "            \"would've\": \"would have\",\n",
    "            \"wouldn't\": \"would not\",\n",
    "            \"wouldn't've\": \"would not have\",\n",
    "            \"y'all\": \"you all\",\n",
    "            \"y'all'd\": \"you all would\",\n",
    "            \"y'all'd've\": \"you all would have\",\n",
    "            \"y'all're\": \"you all are\",\n",
    "            \"y'all've\": \"you all have\",\n",
    "            \"you'd\": \"you would\",\n",
    "            \"you'd've\": \"you would have\",\n",
    "            \"you'll\": \"you will\",\n",
    "            \"you'll've\": \"you will have\",\n",
    "            \"you're\": \"you are\",\n",
    "            \"you've\": \"you have\"\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"The constructor. Your subclass's constructor can add\n",
    "        other arguments.\"\"\"\n",
    "        \n",
    "    def cleanData(self, text, removeStopwords = True):\n",
    "        \"\"\"\n",
    "        This method is a standard implementation to clean any text that are\n",
    "        passed in as parameter. Here the text is split into sentences and each\n",
    "        sentence is in turn cleaned by invoking the cleanSentence() method.\n",
    "        \n",
    "        Any custom cleaning needs to be done at the subclass Preprocessor and\n",
    "        the invoke this method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : string\n",
    "            The text to be cleaned.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        string\n",
    "            The cleaned text.\n",
    "        punctuationsToBeExcluded : list\n",
    "            List of any particular punctuations to be ignored when cleaning \n",
    "            the sentence.\n",
    "\n",
    "        \"\"\"\n",
    "        cleanedSentences = list()\n",
    "        sentences = text.split('\\n')\n",
    "        for sentence in sentences:\n",
    "            # Cleaning the sentence here\n",
    "            sentence = self.cleanSentence(sentence, removeStopwords)\n",
    "            if len(sentence) > 0:\n",
    "                cleanedSentences.append(sentence)\n",
    "        return ' '.join(cleanedSentences).lower()\n",
    "        \n",
    "    def cleanSentence(self, sentence, removeStopwords):\n",
    "        \"\"\"\n",
    "        The method cleans a passed in sentence parameter by:\n",
    "            i. removing all whitespace characters.\n",
    "            ii. removing all punctuations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sentence : string\n",
    "            The sentence to be cleaned.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        string\n",
    "            The cleaned sentence.\n",
    "\n",
    "        \"\"\"\n",
    "        sentence = sentence.lower()\n",
    "        sentence = self.fixContractions(sentence)\n",
    "        sentence = self.removeUnwantedCharacters(sentence)\n",
    "        if removeStopwords:\n",
    "            sentence = self.removeStopWords(sentence)\n",
    "        return sentence\n",
    "    \n",
    "    def fixContractions(self, text, contractionList=CONTRACTION_LIST):\n",
    "        \"\"\"\n",
    "        # Expands the contractions by finding a match in the Contraction list \n",
    "        Regular expression pattern matching.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : string\n",
    "            The text where contractions need to be fixed.\n",
    "        contraction_list : dictionary, optional\n",
    "            The dictionary which tells the mapping for different types of \n",
    "            contractions. The default is CONTRACTION_LIST.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        string\n",
    "            The expanded text.\n",
    "\n",
    "        \"\"\"\n",
    "        text = re.findall(r\"[\\w']+\", text)\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractionList:\n",
    "                new_text.append(contractionList[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        return ' '.join(new_text)\n",
    "    \n",
    "    def removeUnwantedCharacters(self, text):\n",
    "        \"\"\"\n",
    "        Removes all unwanted characters from the text.\n",
    "        This includes any URLs, HTML tags, punctuations, line breaks.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : string\n",
    "            The text that needs to be cleaned.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        text : string\n",
    "            The cleaned text.\n",
    "\n",
    "        \"\"\"\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)# remove links\n",
    "        text = re.sub(r'\\<a href', ' ', text)# remove html link tag\n",
    "        text = re.sub(r'&amp;', '', text) \n",
    "        text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "        text = re.sub(r'<br />', ' ', text)\n",
    "        text = re.sub(r'\\'', ' ', text)\n",
    "        return text\n",
    "    \n",
    "    def removeStopWords(self, text):\n",
    "        \"\"\"\n",
    "        Removes the stop words.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : string\n",
    "            The text where the stop words need to be removed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        string\n",
    "            The stop words removed text.\n",
    "\n",
    "        \"\"\"\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnPreprocessor(BasePreprocessor):\n",
    "    \"\"\"This is a preprocessor class which implements CNN dataset specific\n",
    "    cleaning methods.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor method to do any initial value setting.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        CnnProcessor class object.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "            \n",
    "    def stripOffNewsSource(self, text):\n",
    "        \"\"\"\n",
    "        This method helps to strip off the news source from the text.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : string\n",
    "            The news text.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        text : string\n",
    "            The news text with any news source stripped off.\n",
    "\n",
    "        \"\"\"\n",
    "        closingBracketIndex = text.find(')')\n",
    "        firstWord = ''\n",
    "        if closingBracketIndex > -1:\n",
    "            firstWordToBeExcluded = False\n",
    "            countOfSpaceChar = 0\n",
    "            for i in range(closingBracketIndex-1,-1,-1):\n",
    "                if text[i] == ' ':\n",
    "                    if countOfSpaceChar < 4:\n",
    "                        countOfSpaceChar += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        firstWordToBeExcluded = False\n",
    "                        break\n",
    "                elif text[i] == '(' and not firstWordToBeExcluded:\n",
    "                    countOfSpaceChar = 0\n",
    "                    firstWordToBeExcluded = True\n",
    "            \n",
    "            if firstWordToBeExcluded:\n",
    "                firstWord = text[:closingBracketIndex + 1]\n",
    "                text = text[len(firstWord):].strip()\n",
    "        return text\n",
    "    \n",
    "    def cleanData(self, text, isSummary):\n",
    "        \"\"\"\n",
    "        This method helps to clean any text by calling the cleanData from the base\n",
    "        class. \n",
    "        \n",
    "        The CNN dataset files can have the source of the news at the start of\n",
    "        the file in brackets. It iss wise to remove this as part of the cleaning\n",
    "        as this source name doesn't help with the actual summarisation task.\n",
    "        Hence another method called stripOffNewsSource() is invoked before\n",
    "        before calling the cleanData() method in the base class.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        text : string\n",
    "            The text to be cleaned.\n",
    "        isSummary : boolean\n",
    "            Denotes whether the text to be cleaned is actual News text or \n",
    "            the summary.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        string\n",
    "            The cleaned text.\n",
    "\n",
    "        \"\"\"\n",
    "        # If the text is not a summary, then strip of the news source from\n",
    "        # the text\n",
    "        if not isSummary:\n",
    "            text = self.stripOffNewsSource(text)\n",
    "        \n",
    "        # Invoking the standard cleanData method.\n",
    "        return super().cleanData(text, not isSummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of base class for the data loader.\n",
    "\"\"\"\n",
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    Class to help with the loading of data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cleanDataOp):\n",
    "        \"\"\"\n",
    "        The constructor method to do any initial value setting.\n",
    "        \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        DataLoader class object.\n",
    "\n",
    "        \"\"\"\n",
    "        self.cleanDataOp = cleanDataOp\n",
    "    \n",
    "    def loadSourceDocument(self, filePath):\n",
    "        \"\"\"\n",
    "        Loads the contents of a single source document\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filePath : string\n",
    "            The file path of the source document.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        text : string\n",
    "            The loaded text.\n",
    "\n",
    "        \"\"\"\n",
    "        file = open(filePath, encoding='utf-8')\n",
    "        text = file.read()\n",
    "        file.close()\n",
    "        return text\n",
    "        \n",
    "    def loadSourceDocuments(self, sourceDirectoryPath, refreshSourceDocs):\n",
    "        \"\"\"\n",
    "        This method helps to load the source documents.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sourceDirectoryPath : string\n",
    "            Directory path where the source files reside.\n",
    "        refreshSourceDocs : bool\n",
    "            If this parameter is true, all the source files are read fresh else\n",
    "            already pickled file is loaded.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List of dictionaries holding the loaded text and summaries.\n",
    "\n",
    "        \"\"\"\n",
    "        all_text = {}\n",
    "        all_text['Text'] = []\n",
    "        all_text['Summary'] = []\n",
    "        if refreshSourceDocs:\n",
    "            fileIndex = 1\n",
    "            for name in listdir(sourceDirectoryPath):\n",
    "                if not name.startswith('._'):\n",
    "                    filePath = sourceDirectoryPath + '/' + name\n",
    "                    # load document\n",
    "                    doc = self.loadSourceDocument(filePath)\n",
    "                    text, summary = self.retrieveTextAndSummary(doc)\n",
    "                    all_text['Text'].append(self.cleanDataOp(text, False))\n",
    "                    all_text['Summary'].append(self.cleanDataOp(summary, True))\n",
    "                    print('Extracted and cleaned file number', fileIndex, '=>', name)\n",
    "                    fileIndex += 1\n",
    "        return all_text\n",
    "        \n",
    "    def retrieveTextAndSummary(self, document):\n",
    "        \"\"\"\n",
    "        This method helps separate the actual text and summary from the whole\n",
    "        CNN news document.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        document : string\n",
    "            The content of the news story file from which the actual text and\n",
    "            summary needs to be separated.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        string\n",
    "            The text and a list of summaries.\n",
    "\n",
    "        \"\"\"\n",
    "        # All the summaries in the document are starting with the '@highlight'\n",
    "        # phrase.\n",
    "        textIndex = document.find('@highlight')\n",
    "        \n",
    "        # Splitting the actual text content and the summary lines\n",
    "        text, summaries = document[:textIndex], document[textIndex:].split('@highlight')\n",
    "        \n",
    "        # Stripping all the whitespaces from each of the summary lines.\n",
    "        summaries = [s.strip() for s in summaries if len(s) > 0]\n",
    "        \n",
    "        # Returning the actual text and the list of summaries\n",
    "        return text, ' '.join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of base class for the Word Embedding framework.\n",
    "\"\"\"\n",
    "class WordEmbeddingBase:\n",
    "    \"\"\"The base class for Word Embedding framework.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embeddingsDimension, specialTokens):\n",
    "        \"\"\"The constructor. Your subclass's constructor can add\n",
    "        other arguments.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        WordEmbeddingBase object.\n",
    "\n",
    "        \"\"\"\n",
    "        self.embeddingsDimension = embeddingsDimension\n",
    "        self.specialTokens = specialTokens\n",
    "    \n",
    "    def constructEmbeddingsIndex(self):\n",
    "        \"\"\"\n",
    "        The method to build the embedding index using the vector file\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        embedding_index : dictionary\n",
    "            The word to vector data mapping.\n",
    "\n",
    "        \"\"\"\n",
    "        embeddingsIndex = {}\n",
    "        with codecs.open(self.vectorFilePath, 'r', 'utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                sr = line.split()\n",
    "                word = sr[0]\n",
    "                embedding = np.asarray(sr[1:], dtype='float32')\n",
    "                embeddingsIndex[word] = embedding\n",
    "        return embeddingsIndex\n",
    "        \n",
    "    def buildEmbeddingsVectorMatrix(self, wordToIntDict, embeddingsIndex):\n",
    "        \"\"\"\n",
    "        The method to build the embedding index using the vector file\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        embeddingDimension : number\n",
    "            The dimension of embedding used.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        embeddingMatrix : dictionary\n",
    "            The mapping from integer representation of the word to the \n",
    "            embedding vector.\n",
    "\n",
    "        \"\"\"\n",
    "        embeddingsMatrix = np.zeros((len(wordToIntDict), self.embeddingsDimension), dtype=np.float32)\n",
    "        for word, i in wordToIntDict.items():\n",
    "            embeddingsVector = embeddingsIndex.get(word)\n",
    "            if embeddingsVector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                embeddingsMatrix[i] = embeddingsVector\n",
    "            else:\n",
    "                randomGeneratedEmbeddingsVector = np.array(np.random.uniform(-1.0, 1.0, self.embeddingsDimension))\n",
    "                embeddingsIndex[word] = randomGeneratedEmbeddingsVector\n",
    "                embeddingsMatrix[i] = randomGeneratedEmbeddingsVector\n",
    "        return embeddingsMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of custom class for the Glove Word Embedding framework.\n",
    "\"\"\"\n",
    "class GloveEmbedding(WordEmbeddingBase):\n",
    "    \"\"\"The custom class for Glove Word Embedding framework.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embeddingsDimension, specialTokens):\n",
    "        \"\"\"\n",
    "        The constructor to do any initial value setting.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        GloveEmbedding class object.\n",
    "\n",
    "        \"\"\"\n",
    "        self.vectorFilePath = 'embeddings/frameworks/glove.6B.50d.txt'\n",
    "        super().__init__(embeddingsDimension, specialTokens)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of custom class for the Conceptnet Numberbatch's Embedding framework.\n",
    "\"\"\"\n",
    "class ConceptNetEmbedding(WordEmbeddingBase):\n",
    "    \"\"\"The custom class for Coneptnet Numberbatch's Embedding framework.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embeddingsDimension, specialTokens):\n",
    "        \"\"\"\n",
    "        The constructor to do any initial value setting.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        GloveEmbedding class object.\n",
    "\n",
    "        \"\"\"\n",
    "        self.vectorFilePath = 'embeddings/frameworks/numberbatch-en-19.08.txt'\n",
    "        super().__init__(embeddingsDimension, specialTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utils:\n",
    "    \"\"\"A Utility class for some static helper methods\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def pickle(filename, contents):\n",
    "        \"\"\"\n",
    "        This method pickles the contents to a file\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : string\n",
    "            The pickle file location.\n",
    "        contents : string\n",
    "            The contents to be pickled.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        file = open(filename, \"wb\")\n",
    "        pickle.dump(contents, file)\n",
    "        file.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def unPickle(filename):\n",
    "        \"\"\"\n",
    "        This method loads the contents from a pickled file\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : string\n",
    "            The pickle file location.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The contents from a pickled file.\n",
    "\n",
    "        \"\"\"\n",
    "        file = open(filename,\"rb\")\n",
    "        contents = pickle.load(file)\n",
    "        file.close()\n",
    "        return contents\n",
    "    \n",
    "    @staticmethod\n",
    "    def countWords(wordsCountDict, text):\n",
    "        \"\"\"\n",
    "        This method returns a dictionary with the words to number of occurrences\n",
    "        mapping.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        wordsCountDict : dictionary\n",
    "            Word to number of occurrences mapping.\n",
    "        text : string\n",
    "            The text.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        for sentence in text:\n",
    "            for word in sentence.split():\n",
    "                if word not in wordsCountDict:\n",
    "                    wordsCountDict[word] = 1\n",
    "                else:\n",
    "                    wordsCountDict[word] += 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def buildWordToNumberRepresentations(wordsCountDict, specialTokens, embeddingsIndex, thresholdForRareWordsCount):\n",
    "        \"\"\"\n",
    "        This method returns two dictionaries with a word to number mapping and another one with number to word \n",
    "        mapping.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        wordsCountDict : dictionary\n",
    "            Word to number of occurrences mapping.\n",
    "        specialTokens: dictionary\n",
    "            Special tokens to number mapping\n",
    "        embeddingsIndex: dictionary\n",
    "            The dictionary which has the mapping from a word to corresponding embedding vector. This dictionary\n",
    "            is normally constructed from a word embeddings vector file.\n",
    "        thresholdForRareWordsCount : int\n",
    "            Only those words with frequencies above this threshold are considered if they are not part of\n",
    "            the embeddings index dictionary.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Two dictionaries:\n",
    "            i. Word to Number mapping\n",
    "            ii. Number to Word mapping\n",
    "\n",
    "        \"\"\"\n",
    "        wordToIntDict = {}\n",
    "        intToWordDict = {}\n",
    "        wordIndex = 0\n",
    "        for word, count in wordsCountDict.items():\n",
    "            if count >= thresholdForRareWordsCount or word in embeddingsIndex:\n",
    "                wordToIntDict[word] = wordIndex\n",
    "                intToWordDict[wordIndex] = word\n",
    "                wordIndex += 1\n",
    "        \n",
    "        for token in specialTokens.values():\n",
    "            wordToIntDict[token] = wordIndex\n",
    "            intToWordDict[wordIndex] = token\n",
    "            wordIndex += 1\n",
    "        \n",
    "        return wordToIntDict, intToWordDict\n",
    "    \n",
    "    @staticmethod\n",
    "    def convertTextToNumberSequence(text, wordToIntDict, unknownToken, eosToken = None, applyEos = False):\n",
    "        \"\"\"\n",
    "        This method converts a text to a sequence of numbers based on the word to integer mapping dictionary.\n",
    "        If a word does not exist in the word to integer mapping dictionary, a number representation of 'Unknown'\n",
    "        special token is used instead.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        wordToIntDict : dictionary\n",
    "            Word to number of  mapping.\n",
    "        unknownToken: string\n",
    "            The 'Unknown' specal token string.\n",
    "        eosToken: number\n",
    "            The 'End of Sequence' special token string.\n",
    "        applyEos : boolean\n",
    "            If true, at the end of the number sequence the number corresponding to 'End of Sequence' special token\n",
    "            shall be appended. \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        i. The sequence of numbers\n",
    "        ii. Total words count\n",
    "        iii. Total unknown words count\n",
    "        \"\"\"\n",
    "        numberSequenceForText = []\n",
    "        wordsCount = 0\n",
    "        unknownWordsCount = 0\n",
    "        for sentence in text:\n",
    "            numberSequenceForSentence = []\n",
    "            for word in sentence.split():\n",
    "                wordsCount += 1\n",
    "                if word in wordToIntDict:\n",
    "                    numberSequenceForSentence.append(wordToIntDict[word])\n",
    "                else:\n",
    "                    numberSequenceForSentence.append(wordToIntDict[unknownToken])\n",
    "                    unknownWordsCount += 1\n",
    "            \n",
    "            if applyEos and eosToken is not None:\n",
    "                numberSequenceForSentence.append(wordToIntDict[eosToken])\n",
    "            numberSequenceForText.append(numberSequenceForSentence)\n",
    "        return numberSequenceForText, wordsCount, unknownWordsCount\n",
    "    \n",
    "    @staticmethod       \n",
    "    def applyFilterAndSort(summariesAndTextZippedList, summaryAndTextAttributes):\n",
    "        \"\"\"\n",
    "        Filter method to filter out summary and text zipped entry based on maximum Summary Length, \n",
    "        maximum Text length, unknown word limit in summaries and unknown word limit in text.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        summariesAndTextZippedList: list\n",
    "            List of zipped version of Summary and Text\n",
    "        summaryAndTextAttributes : dictionary\n",
    "            Carries:\n",
    "                i. The maximum number of words allowed in a Summary\n",
    "                ii. The maximum number of words allowed in a Text\n",
    "                i. The minimum number of words required in a Summary\n",
    "                ii. The minimum number of words required in a Text\n",
    "                iii. The maximum number of unknown words allowed in a Summary\n",
    "                iv. The maximum number of unknown words allowed in a Text\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        i. The sequence of numbers\n",
    "        ii. Total words count\n",
    "        iii. Total unknown words count\n",
    "        \"\"\"\n",
    "        maximumSummaryLength = summaryAndTextAttributes['maximumSummaryLength']\n",
    "        maximumTextLength = summaryAndTextAttributes['maximumTextLength']\n",
    "        minimumSummaryLength = summaryAndTextAttributes['minimumSummaryLength']\n",
    "        minimumTextLength = summaryAndTextAttributes['minimumTextLength']\n",
    "        unknownsInSummaryLimit = summaryAndTextAttributes['unknownsInSummaryLimit']\n",
    "        unknownsInTextLimit = summaryAndTextAttributes['unknownsInTextLimit']\n",
    "        unknownTokenNumberRepresentation = summaryAndTextAttributes['unknownTokenNumberRepresentation']\n",
    "        \n",
    "        def countUnknowns(sentence, unknownTokenNumberRepresentation):\n",
    "            '''Counts the number of time UNK appears in a sentence.'''\n",
    "            unknownsCount = 0\n",
    "            for word in sentence:\n",
    "                if word == unknownTokenNumberRepresentation:\n",
    "                    unknownsCount += 1\n",
    "            return unknownsCount\n",
    "    \n",
    "        def filterCondition(item):\n",
    "            \"\"\"\n",
    "            Filters an item based on certain conditions.\n",
    "            \"\"\"\n",
    "            summarySeq = item[0]\n",
    "            textSeq = item[1]\n",
    "            if(len(summarySeq) <= maximumSummaryLength and\n",
    "               len(textSeq) <= maximumTextLength and \n",
    "               len(summarySeq) >= minimumSummaryLength and\n",
    "               len(textSeq) >= minimumTextLength and\n",
    "               countUnknowns(summarySeq, unknownTokenNumberRepresentation) <= unknownsInSummaryLimit and \n",
    "               countUnknowns(textSeq, unknownTokenNumberRepresentation) <= unknownsInTextLimit):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    \n",
    "        filteredSummariesAndText = list(filter(filterCondition, summariesAndTextZippedList))\n",
    "        summariesAndTextSorted = sorted(filteredSummariesAndText, key=lambda entry: len(entry[1]))\n",
    "        summariesAndTextSorted = list(zip(*summariesAndTextSorted))\n",
    "        return list(summariesAndTextSorted[0]), list(summariesAndTextSorted[1])\n",
    "    \n",
    "    @staticmethod\n",
    "    def computeSequenceLengthsIntoDataFrame(textToNumberSequences):\n",
    "        '''Create a data frame of the sentence lengths from a text'''\n",
    "        lengths = []\n",
    "        for textToNumberSequence in textToNumberSequences:\n",
    "            lengths.append(len(textToNumberSequence))\n",
    "        return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel:\n",
    "    \"\"\"\n",
    "    The implementation for Sequence to sequence modelling\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"The constructor. Your subclass's constructor can add\n",
    "        other arguments.\"\"\"\n",
    "        \n",
    "    def createModelInputsPlaceholders(self):\n",
    "        inputData = tf.placeholder(tf.int32, [None, None], name='inputData')\n",
    "        targetData = tf.placeholder(tf.int32, [None, None], name='targetData')\n",
    "        learningRate = tf.placeholder(tf.float32, name='learningRate')\n",
    "        dropoutRate = tf.placeholder(tf.float32, name='dropoutRate')\n",
    "        inputSummaryLengths = tf.placeholder(tf.int32, (None,), name='inputSummaryLengths')\n",
    "        maximumSummaryLength = tf.reduce_max(inputSummaryLengths, name='maximumSummaryLength')\n",
    "        inputTextLengths = tf.placeholder(tf.int32, (None,), name='inputTextLengths')\n",
    "\n",
    "        return inputData, targetData, learningRate, dropoutRate, inputSummaryLengths, maximumSummaryLength, inputTextLengths\n",
    "        \n",
    "    def createLSTMCell(self, rnnPerCellUnitsCount, requireDropoutLayer = False, dropoutRate = 0.95):\n",
    "        # Creating the RNN cell\n",
    "        cell = tf.contrib.rnn.LSTMCell(rnnPerCellUnitsCount,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        \n",
    "        # Attaching a dropout layer for the cell if required\n",
    "        if requireDropoutLayer:\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob = dropoutRate)\n",
    "        return cell\n",
    "    \n",
    "    def doEncoding(self, rnnPerCellUnitsCount, inputTextLengths, rnnCellsCount, embeddedEncoderInput, dropoutRate):\n",
    "        \"\"\"\n",
    "        This is the implementation of an encoding process.\n",
    "        \"\"\"\n",
    "        for rnnCellIndex in range(rnnCellsCount):\n",
    "            with tf.variable_scope('encoder_{}'.format(rnnCellIndex)):\n",
    "                # Creating the forward RNN cell for the Bi-directional RNN\n",
    "                forwardCell = self.createLSTMCell(rnnPerCellUnitsCount, \n",
    "                                             requireDropoutLayer = True, \n",
    "                                             dropoutRate = dropoutRate)\n",
    "                \n",
    "                # Creating the backward RNN cell for the Bi-directional RNN\n",
    "                backwardCell = self.createLSTMCell(rnnPerCellUnitsCount, \n",
    "                                             requireDropoutLayer = True, \n",
    "                                             dropoutRate = dropoutRate)\n",
    "                \n",
    "                # Connecting the forward and backward cells to create a Bi-directional RNN\n",
    "                encoderOutput, encoderStates = tf.nn.bidirectional_dynamic_rnn(forwardCell, \n",
    "                                                                    backwardCell, \n",
    "                                                                    embeddedEncoderInput,\n",
    "                                                                    inputTextLengths,\n",
    "                                                                    dtype=tf.float32)\n",
    "                encoderOutput = tf.concat(encoderOutput, 2)\n",
    "                # The current layer's output is being fed into next layer's input\n",
    "                embeddedEncoderInput = encoderOutput\n",
    "        return encoderOutput, encoderStates\n",
    "    \n",
    "    def processDecoderInput(self, targetData, wordToIntDict, batchSize, startToken):\n",
    "        \"\"\"\n",
    "        Remove the last word id from each batch and concatenate the id of the STARTOFSEQUENCE to the \n",
    "        begining of each batch.\n",
    "        \"\"\"\n",
    "        ending = tf.strided_slice(targetData, [0, 0], [batchSize, -1], [1, 1])\n",
    "        decoderInput = tf.concat([tf.fill([batchSize, 1], wordToIntDict[startToken]), ending], 1)\n",
    "        return decoderInput\n",
    "        \n",
    "    def processTrainingLayerForDecoder(self, embeddedDecoderInput, inputSummaryLengths, decoderCell,\n",
    "                                      outputLayer, totalWordsCountInVocab, maximumSummaryLength,\n",
    "                                      batchSize):\n",
    "        \"\"\"\n",
    "        This is the implementation for a Training decoding layer.\n",
    "        \"\"\"\n",
    "        trainingHelper = tf.contrib.seq2seq.TrainingHelper(inputs = embeddedDecoderInput,\n",
    "                                                        sequence_length = inputSummaryLengths,\n",
    "                                                        time_major = False)\n",
    "        \n",
    "        trainingDecoder = tf.contrib.seq2seq.BasicDecoder(cell = decoderCell,\n",
    "                                                       helper = trainingHelper,\n",
    "                                                       initial_state = decoderCell.zero_state(\n",
    "                                                           dtype=tf.float32, batch_size=batchSize),\n",
    "                                                       output_layer = outputLayer)\n",
    "        \n",
    "        trainingLogits = tf.contrib.seq2seq.dynamic_decode(trainingDecoder,\n",
    "                                                           output_time_major = False,\n",
    "                                                           impute_finished = True,\n",
    "                                                           maximum_iterations = maximumSummaryLength)\n",
    "        return trainingLogits\n",
    "        \n",
    "    def processInferenceLayerForDecoder(self, embeddingsMatrix, startOfSequenceToken, endOfSequenceToken,\n",
    "                                       decoderCell, outputLayer, maximumSummaryLength, batchSize):\n",
    "        \"\"\"\n",
    "        This is the implementation for an Inference decoding layer.\n",
    "        \"\"\"\n",
    "        startTokens = tf.tile(tf.constant([startOfSequenceToken], dtype=tf.int32), \n",
    "                              [batchSize], \n",
    "                              name='start_tokens')\n",
    "        \n",
    "        inferenceHelper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddingsMatrix,\n",
    "                                                                   startTokens,\n",
    "                                                                   endOfSequenceToken)\n",
    "        \n",
    "        inferenceDecoder = tf.contrib.seq2seq.BasicDecoder(decoderCell,\n",
    "                                                        inferenceHelper,\n",
    "                                                        decoderCell.zero_state(\n",
    "                                                            dtype=tf.float32, batch_size=batchSize),\n",
    "                                                        outputLayer)\n",
    "        \n",
    "        inferenceLogits = tf.contrib.seq2seq.dynamic_decode(inferenceDecoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=maximumSummaryLength)\n",
    "        \n",
    "        return inferenceLogits\n",
    "    \n",
    "    def doDecoding(self, embeddedDecoderInput, embeddingsMatrix, encoderOutput, encoderStates,\n",
    "                   totalWordsCountInVocab, inputTextLengths, inputSummaryLengths, maximumSummaryLength, \n",
    "                   rnnPerCellUnitsCount, wordToIntDict, dropoutRate, batchSize, rnnCellsCount, \n",
    "                   enableAttention = True):\n",
    "        # Creating the RNN cell for the decoder\n",
    "        decoderCell = tf.contrib.rnn.MultiRNNCell([self.createLSTMCell(rnnPerCellUnitsCount, requireDropoutLayer = True, dropoutRate = dropoutRate) for _ in range(rnnCellsCount)])\n",
    "\n",
    "        # If an additional Attention layer needs to be applied\n",
    "        if enableAttention:\n",
    "            attentionMechanism = tf.contrib.seq2seq.BahdanauAttention(rnnPerCellUnitsCount,\n",
    "                                                     encoderOutput,\n",
    "                                                     inputTextLengths,\n",
    "                                                     normalize = False,\n",
    "                                                     name = 'BahdanauAttention')\n",
    "            decoderCell = tf.contrib.seq2seq.AttentionWrapper(decoderCell, attentionMechanism, rnnPerCellUnitsCount)\n",
    "            \n",
    "        outputLayer = Dense(totalWordsCountInVocab, \n",
    "                            kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "        with tf.variable_scope(\"decode\"):\n",
    "            trainingLogits = self.processTrainingLayerForDecoder(embeddedDecoderInput,\n",
    "                                                            inputSummaryLengths,\n",
    "                                                            decoderCell,\n",
    "                                                            outputLayer,\n",
    "                                                            totalWordsCountInVocab,\n",
    "                                                            maximumSummaryLength,\n",
    "                                                            batchSize)\n",
    "        with tf.variable_scope(\"decode\", reuse=True):\n",
    "            inferenceLogits = self.processInferenceLayerForDecoder(embeddingsMatrix,\n",
    "                                                               wordToIntDict[embedding.specialTokens['STARTOFSEQUENCE']],\n",
    "                                                               wordToIntDict[embedding.specialTokens['ENDOFSEQUENCE']],\n",
    "                                                               decoderCell,\n",
    "                                                               outputLayer,\n",
    "                                                               maximumSummaryLength,\n",
    "                                                               batchSize)\n",
    "        return trainingLogits, inferenceLogits\n",
    "    \n",
    "    def process(self, inputData, targetData, dropoutRate, inputTextLengths, inputSummaryLengths, \n",
    "                maximumSummaryLength, totalWordsCountInVocab, rnnPerCellUnitsCount, \n",
    "                rnnCellsCount, wordToIntDict, batchSize, embeddingsMatrix):\n",
    "        \n",
    "        # Performing parallel lookups of inputData on the embeddingMatrix\n",
    "        embeddedEncoderInput = tf.nn.embedding_lookup(embeddingsMatrix, inputData)\n",
    "        \n",
    "        # Performing the encoding\n",
    "        encoderOutput, encoderStates = self.doEncoding(rnnPerCellUnitsCount,\n",
    "                                                       inputTextLengths,\n",
    "                                                       rnnCellsCount,\n",
    "                                                       embeddedEncoderInput,\n",
    "                                                       dropoutRate)\n",
    "        \n",
    "        # Process the decoder input before passing to decoding layer\n",
    "        decoderInput = self.processDecoderInput(targetData, \n",
    "                                           wordToIntDict, \n",
    "                                           batchSize, \n",
    "                                           embedding.specialTokens['STARTOFSEQUENCE'])\n",
    "        \n",
    "        # Performing parallel lookups of decoder input on the embeddingMatrix\n",
    "        embeddedDecoderInput = tf.nn.embedding_lookup(embeddingsMatrix, decoderInput)\n",
    "        \n",
    "        # Performing the encoding\n",
    "        trainingLogits, inferenceLogits = self.doDecoding(embeddedDecoderInput,\n",
    "                                                     embeddingsMatrix,\n",
    "                                                     encoderOutput,\n",
    "                                                     encoderStates,\n",
    "                                                     totalWordsCountInVocab,\n",
    "                                                     inputTextLengths,\n",
    "                                                     inputSummaryLengths,\n",
    "                                                     maximumSummaryLength,\n",
    "                                                     rnnPerCellUnitsCount,\n",
    "                                                     wordToIntDict,\n",
    "                                                     dropoutRate,\n",
    "                                                     batchSize,\n",
    "                                                     rnnCellsCount)\n",
    "        \n",
    "        return trainingLogits, inferenceLogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchDataGenerator:\n",
    "    \"\"\"\n",
    "    A class which helps in the generation of batches of data\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def generateBatches(summaries, texts, batchSize, paddingToken):\n",
    "        def padBatchContents(contents, paddingToken):\n",
    "            maxContentLength = max([len(content) for content in contents])\n",
    "            return [content + [paddingToken] * (maxContentLength - len(content)) for content in contents]\n",
    "        possibleBatchCount = len(texts)//batchSize\n",
    "        for batchIndex in range(0, possibleBatchCount):\n",
    "            batchStartPoint = batchIndex * batchSize\n",
    "            summariesBatch = summaries[batchStartPoint: batchStartPoint + batchSize]\n",
    "            textBatch = texts[batchStartPoint: batchStartPoint + batchSize]\n",
    "            paddedSummariesBatch = np.array(padBatchContents(summariesBatch, paddingToken))\n",
    "            paddedTextBatch = np.array(padBatchContents(textBatch, paddingToken))\n",
    "            \n",
    "            # Need the lengths for the lengths parameters\n",
    "            paddedSummariesLength = []\n",
    "            for summary in paddedSummariesBatch:\n",
    "                paddedSummariesLength.append(len(summary))\n",
    "\n",
    "            paddedTextLength = []\n",
    "            for text in paddedTextBatch:\n",
    "                paddedTextLength.append(len(text))\n",
    "\n",
    "            yield paddedSummariesBatch, paddedTextBatch, paddedSummariesLength, paddedTextLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "sourceDirectoryPath = '../data/cnn/stories'\n",
    "refreshSourceDocs = False\n",
    "pickledFilePath = '../data/cnn_dataset.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Texts 92579\n"
     ]
    }
   ],
   "source": [
    "if refreshSourceDocs:\n",
    "    preprocessor = CnnPreprocessor()\n",
    "    dataLoader = DataLoader(preprocessor.cleanData)\n",
    "    loadedContent = dataLoader.loadSourceDocuments(sourceDirectoryPath, refreshSourceDocs)\n",
    "                \n",
    "    # save to file\n",
    "    Utils.pickle(pickledFilePath, loadedContent)\n",
    "    print('Pickled the cleaned data into the file:', pickledFilePath)\n",
    "\n",
    "# load from file\n",
    "news = Utils.unPickle(pickledFilePath)\n",
    "print('Loaded Texts %d' % len(news['Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedText = news['Text']\n",
    "cleanedSummaries = news['Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the word embedding class\n",
    "embeddingsDimension = 50\n",
    "specialTokens = {\n",
    "    'UNKNOWN': '<UNK>',\n",
    "    'PADDING': '<PAD>',\n",
    "    'ENDOFSEQUENCE': '<EOS>',\n",
    "    'STARTOFSEQUENCE': '<GO>'\n",
    "}\n",
    "embedding = GloveEmbedding(embeddingsDimension, specialTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 238749\n"
     ]
    }
   ],
   "source": [
    "# Creating a dictionary with word to frequency mapping\n",
    "wordsCountDict = {}\n",
    "Utils.countWords(wordsCountDict, cleanedText)\n",
    "Utils.countWords(wordsCountDict, cleanedSummaries)\n",
    "print(\"Size of Vocabulary:\", len(wordsCountDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a word embeddings index\n",
    "# This is simply a word to word vector mapping dictionary\n",
    "embeddingsIndex = embedding.constructEmbeddingsIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddingsIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This value defines the threshold of the minimum number of occurrences of an Unknown word for that word\n",
    "# to be included in the word to number representation dictionary.\n",
    "thresholdForRareWordsCount = 10\n",
    "\n",
    "# Building the word to number representation dictionary for representing a big text as a sequence of numbers\n",
    "# when passed to the RNN\n",
    "# Alone with this a number to word representation dictionary is also built which helps in the conversion of final\n",
    "# output of sequence of numbers to corresponding Predicted summary text.\n",
    "wordToIntDict, intToWordDict = Utils.buildWordToNumberRepresentations(\n",
    "    wordsCountDict, embedding.specialTokens, embeddingsIndex, thresholdForRareWordsCount\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of embeddings: 158180\n"
     ]
    }
   ],
   "source": [
    "# Building the Embeddings vector matrix which is basically a two dimensional matrix with\n",
    "#    Number of rows = number of words in wordtoIntDict above\n",
    "#    Number of columns = the dimensionality of chosen word embedding framework (each word vector will be of this size)\n",
    "# Also if there are any unknown words in workToIntDict which are not there in the embeddingsIndex constructured from\n",
    "# the word embedding file, a random word vector shall be generated and inserted as a row in the \n",
    "# Embeddings vector matrix.\n",
    "embeddingsMatrix = embedding.buildEmbeddingsVectorMatrix(wordToIntDict, embeddingsIndex)\n",
    "print('Total number of embeddings:', len(embeddingsMatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 39010724\n",
      "Total number of UNKs: 150476\n",
      "Percent of words that are UNK: 0.38999999999999996%\n"
     ]
    }
   ],
   "source": [
    "# Converting all the summaries to corresponding number sequences\n",
    "summariesToNumberSequence, summaryWordsCount, summaryUnknownWordsCount = Utils.convertTextToNumberSequence(\n",
    "    cleanedSummaries, \n",
    "    wordToIntDict, \n",
    "    embedding.specialTokens['UNKNOWN']\n",
    ")\n",
    "\n",
    "# Converting all the text to corresponding number sequences\n",
    "textToNumberSequence, textWordsCount, textUnknownWordsCount = Utils.convertTextToNumberSequence(\n",
    "    cleanedText, \n",
    "    wordToIntDict, \n",
    "    embedding.specialTokens['UNKNOWN'], \n",
    "    eosToken = embedding.specialTokens['ENDOFSEQUENCE'],\n",
    "    applyEos = True\n",
    ")\n",
    "\n",
    "totalWordsCount = summaryWordsCount + textWordsCount\n",
    "totalUnknownWordsCount = summaryUnknownWordsCount + textUnknownWordsCount\n",
    "unknownPercentage = round(totalUnknownWordsCount/totalWordsCount,4) * 100\n",
    "\n",
    "print(\"Total number of words:\", totalWordsCount)\n",
    "print(\"Total number of UNKs:\", totalUnknownWordsCount)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unknownPercentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464.0\n",
      "645.0\n",
      "748.0\n",
      "927.2200000000012\n",
      "49.0\n",
      "56.0\n",
      "59.0\n",
      "67.0\n"
     ]
    }
   ],
   "source": [
    "lengthSummaries = Utils.computeSequenceLengthsIntoDataFrame(summariesToNumberSequence)\n",
    "lengthText = Utils.computeSequenceLengthsIntoDataFrame(textToNumberSequence)\n",
    "\n",
    "# Inspect the length of texts\n",
    "print(np.percentile(lengthText.counts, 70))\n",
    "print(np.percentile(lengthText.counts, 90))\n",
    "print(np.percentile(lengthText.counts, 95))\n",
    "print(np.percentile(lengthText.counts, 99))\n",
    "\n",
    "# Inspect the length of summaries\n",
    "print(np.percentile(lengthSummaries.counts, 70))\n",
    "print(np.percentile(lengthSummaries.counts, 90))\n",
    "print(np.percentile(lengthSummaries.counts, 95))\n",
    "print(np.percentile(lengthSummaries.counts, 99.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64603\n",
      "64603\n"
     ]
    }
   ],
   "source": [
    "maximumTextLength = 464\n",
    "maximumSummaryLength = 67\n",
    "minimumTextLength = 2\n",
    "minimumSummaryLength = 2\n",
    "unknownsInSummaryLimit = 4\n",
    "unknownsInTextLimit = 10\n",
    "        \n",
    "summariesAndTextSequence = list(zip(summariesToNumberSequence, textToNumberSequence))\n",
    "sortedSummaries, sortedText = Utils.applyFilterAndSort(summariesAndTextSequence, {\n",
    "    'maximumTextLength': maximumTextLength,\n",
    "    'maximumSummaryLength': maximumSummaryLength,\n",
    "    'minimumTextLength': minimumTextLength,\n",
    "    'minimumSummaryLength': minimumSummaryLength,\n",
    "    'unknownsInSummaryLimit': unknownsInSummaryLimit,\n",
    "    'unknownsInTextLimit': unknownsInTextLimit,\n",
    "    'unknownTokenNumberRepresentation': embedding.specialTokens['UNKNOWN']\n",
    "})\n",
    "\n",
    "# Compare lengths to ensure they match\n",
    "print(len(sortedSummaries))\n",
    "print(len(sortedText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utils.pickle(\"../data/sorted_summaries.pkl\",sortedSummaries)\n",
    "Utils.pickle(\"../data/sorted_text.pkl\",sortedText)\n",
    "Utils.pickle(\"../data/embeddings_matrix.pkl\",embeddingsMatrix)\n",
    "Utils.pickle(\"../data/word_to_int.pkl\",wordToIntDict)\n",
    "Utils.pickle(\"../data/int_to_word.pkl\",intToWordDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedSummaries = Utils.unPickle(\"../data/sorted_summaries.pkl\")\n",
    "sortedText = Utils.unPickle(\"../data/sorted_text.pkl\")\n",
    "embeddingsMatrix = Utils.unPickle(\"../data/embeddings_matrix.pkl\")\n",
    "wordToIntDict = Utils.unPickle(\"../data/word_to_int.pkl\")\n",
    "intToWordDict = Utils.unPickle(\"../data/int_to_word.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 100\n",
    "batchSize = 15\n",
    "rnnPerCellUnitsCount = 128\n",
    "rnnCellsCount = 2\n",
    "learningRate = 0.001\n",
    "dropoutRate = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f9b7cf81c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f9b7cf81c50>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f9b7cf81c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f9b7cf81c50>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f9b7cf81f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f9b7cf81f28>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f9b7cf81f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f9b7cf81f28>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f9b7cf81358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f9b7cf81358>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f9b7cf81358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f9b7cf81358>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f974e914eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f974e914eb8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f974e914eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f974e914eb8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f974ef6cf60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f974ef6cf60>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f974ef6cf60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f974ef6cf60>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f974eea5780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f974eea5780>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f974eea5780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f974eea5780>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f974e8d2a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f974e8d2a20>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f974e8d2a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f974e8d2a20>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f974eb7a240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f974eb7a240>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f974eb7a240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f974eb7a240>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f974ef6cf60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f974ef6cf60>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f974ef6cf60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionWrapper.call of <tensorflow.contrib.seq2seq.python.ops.attention_wrapper.AttentionWrapper object at 0x7f974ef6cf60>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f974eea5780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f974eea5780>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f974eea5780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f974eea5780>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f974e8d2a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f974e8d2a20>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f974e8d2a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f974e8d2a20>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f974eb7a240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f974eb7a240>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f974eb7a240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f974eb7a240>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f974ef2c9b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Graph is built.\n",
      "../modelrun/graph\n"
     ]
    }
   ],
   "source": [
    "seq2seqModel = Seq2SeqModel()\n",
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, dropout_rate, summary_length, max_summary_length, text_length = seq2seqModel.createModelInputsPlaceholders()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    trainingLogits, inferenceLogits = seq2seqModel.process(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      dropout_rate,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(wordToIntDict)+1,\n",
    "                                                      rnnPerCellUnitsCount, \n",
    "                                                      rnnCellsCount, \n",
    "                                                      wordToIntDict,\n",
    "                                                      batchSize,\n",
    "                                                      embeddingsMatrix)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    trainingLogits = tf.identity(trainingLogits[0].rnn_output, 'logits')\n",
    "    inferenceLogits = tf.identity(inferenceLogits[0].sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss, the sould be all True across since each batch is padded\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            trainingLogits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learningRate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")\n",
    "graph_location = \"../modelrun/graph\"\n",
    "print(graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(train_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64603\n",
      "3000\n",
      "The shortest text length: 44\n",
      "The longest text length: 345\n"
     ]
    }
   ],
   "source": [
    "# Subset the data for training\n",
    "start = 150\n",
    "end = start + 45000\n",
    "print(len(sortedSummaries))\n",
    "sampledSortedSummaries = sortedSummaries[start:end:15]\n",
    "sampledSortedText = sortedText[start:end:15]\n",
    "print(len(sampledSortedSummaries))\n",
    "print(\"The shortest text length:\", len(sampledSortedText[0]))\n",
    "print(\"The longest text length:\",len(sampledSortedText[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/100 Batch   10/200 - Loss:  9.555, Seconds: 89.22\n",
      "Epoch   1/100 Batch   20/200 - Loss:  7.760, Seconds: 63.90\n",
      "Epoch   1/100 Batch   30/200 - Loss:  6.529, Seconds: 91.88\n",
      "Epoch   1/100 Batch   40/200 - Loss:  6.179, Seconds: 105.31\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 10 # Check training loss after every 10 batches\n",
    "stop_early = 0 \n",
    "stop = 2 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 2 # Make 2 update checks per epoch\n",
    "update_check = (len(sampledSortedText)//batchSize//per_epoch)\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "paddingToken = wordToIntDict[embedding.specialTokens['PADDING']]\n",
    "checkpoint = \"../modelrun/best_model.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
    "    #loader.restore(sess, checkpoint)\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                BatchDataGenerator.generateBatches(sampledSortedSummaries, sampledSortedText, batchSize, paddingToken)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learningRate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 dropout_rate: dropoutRate})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if (batch_i+1) % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i+1, \n",
    "                              len(sampledSortedText) // batchSize, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if (batch_i+1) % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learningRate *= learning_rate_decay\n",
    "        if learningRate < min_learning_rate:\n",
    "            learningRate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsIndex = 165\n",
    "totalNewsCount = len(textToNumberSequence)\n",
    "testNews = [textToNumberSequence[newsIndex]]\n",
    "maxSummaryLength = len(news['Summary'][newsIndex])\n",
    "print(testNews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"./best_model.ckpt\"\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "    input_data = loaded_graph.get_tensor_by_name('inputData:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('inputTextLengths:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('inputSummaryLengths:0')\n",
    "    dropout_rate = loaded_graph.get_tensor_by_name('dropoutRate:0')\n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    for i, text in enumerate(testNews):\n",
    "        answer_logits = sess.run(logits, {input_data: [text]*batchSize, \n",
    "                                          summary_length: [maxSummaryLength], #summary_length: [np.random.randint(5,8)], \n",
    "                                          text_length: [len(text)]*batchSize,\n",
    "                                          dropout_rate: 1.0})[0] \n",
    "        # Remove the padding from the summaries\n",
    "        pad = wordToIntDict[\"<PAD>\"] \n",
    "        #print('- News:\\n\\r {}\\n\\r\\n\\r'.format(\" \".join([intToWordDict[j] for j in testNews[i] if j != pad])))\n",
    "        print('- News:\\n\\r {}\\n\\r\\n\\r'.format(news['Text'][newsIndex]))\n",
    "        print('- Actual Summary:\\n\\r {}\\n\\r\\n\\r'.format(news['Summary'][newsIndex]))\n",
    "        print('- Predicted Summary:\\n\\r {}\\n\\r\\n\\r'.format(\" \".join([intToWordDict[j] for j in answer_logits if j != pad])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
